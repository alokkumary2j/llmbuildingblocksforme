{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    bias: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"decoder_mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "        #Reshaping buffer as (1 ~~> batch_size, 1 ~~~> num_heads, block_size, block_size)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        qkv_transformations= self.c_attn(x) # batch_size, sequence_length, 3 * n_embd\n",
    "        q, k, v  = qkv_transformations.split(self.n_embd, dim=2)\n",
    "\n",
    "        q_rearranged= q.view(B, T, self.n_head, C // self.n_head) # (B,T,n_heads,partitioned_head_size)\n",
    "        k_rearranged= k.view(B, T, self.n_head, C // self.n_head) # (B,T,n_heads,partitioned_head_size)\n",
    "        v_rearranged= v.view(B, T, self.n_head, C // self.n_head) # (B,T,n_heads,partitioned_head_size)\n",
    "\n",
    "        q = q_rearranged.transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k_rearranged.transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v_rearranged.transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) #(B,nh,T,T)\n",
    "        # Without Multi-Head Attention ~~> Attention Weights would have been just (B,T,T)\n",
    "        # However, Remember that the hidden_dimension or inner_dimenion has been reduce from head_size to partitioned_head_size\n",
    "        # for computing each of the B*nh Attention Weight Matrices now!\n",
    "        print_b,print_head,print_time,print_dims=1,0,2,T\n",
    "        print(\" Sample#: {print_b} , Head#: {print_head}, Token# Under Attention Investigation:{print_time}, \"\n",
    "        \"Total Attention Weights for Ref Token: {print_dims}\".format(\n",
    "            print_b=print_b,print_head=print_head,print_time=print_time,print_dims=print_dims))\n",
    "        print(\"Attention Weight Logits Before Self-Maksing \", att[print_b,print_head,print_time,:print_dims])\n",
    "        att = att.masked_fill(self.decoder_mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        print(\"Attention Weight Logits After Decoder-Self Masking \", att[print_b,print_head,print_time,:print_dims])\n",
    "        ############################\n",
    "        if attention_mask is not None:\n",
    "            # Expand mask to match scores shape\n",
    "            #attention_mask.shape ~> (B,T)\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, T]\n",
    "            # mask = (1 - attention_mask) * -1e9  # Invert: 1 -> 0, 0 -> -1e9\n",
    "            # att = att + mask\n",
    "            att = att.masked_fill(attention_mask == 0, float('-inf'))\n",
    "            print(\"Attention Weight Logits After External Attention-Masking \", att[print_b,print_head,print_time,:print_dims])\n",
    "        #############################\n",
    "        #(B,nh,T,T)\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        print(\"Attention Weight Logits After Softmax \", att[print_b,print_head,print_time,:print_dims]*100)\n",
    "        #att: (B,nh,T,T)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        # Note Here; Values are Weighted Separately for Every head for Each Position\n",
    "        # Hence, Total Number of Weighted Values are (B*nh*T); Hoever, the dimension for every Value here has become smaller\n",
    "        # by a factor of num_heads\n",
    "        y_transposed= y.transpose(1,2) #(B, nh, T, hs) ~~> (B, T, nh, hs) \n",
    "        #y_transposed has become non-contiguous because of the transpose operation\n",
    "        y = y_transposed.contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # y is back to (B, T, n_embed) dimension <Same as Input>\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return att,attention_mask,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockEmbeddingModel(nn.Module):\n",
    "    def __init__(self, gptconf, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.wte= nn.Embedding(gptconf.vocab_size,gptconf.n_embd)\n",
    "    def forward(self,x):\n",
    "        return self.wte(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      " Sample#: 1 , Head#: 0, Token# Under Attention Investigation:2, Total Attention Weights for Ref Token: 4\n",
      "Attention Weight Logits Before Self-Maksing  tensor([-0.0410,  0.0649, -0.2804, -0.3085], grad_fn=<SelectBackward0>)\n",
      "Attention Weight Logits After Decoder-Self Masking  tensor([-0.0410,  0.0649, -0.2804,    -inf], grad_fn=<SelectBackward0>)\n",
      "Attention Weight Logits After External Attention-Masking  tensor([   -inf,  0.0649, -0.2804,    -inf], grad_fn=<SelectBackward0>)\n",
      "Attention Weight Logits After Softmax  tensor([ 0.0000, 58.5487, 41.4513,  0.0000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "hidden_size = 4  # Small for demo\n",
    "num_heads = 2\n",
    "head_dim = hidden_size // num_heads  # 2\n",
    "gptconf = GPTConfig()\n",
    "mock_emb=MockEmbeddingModel(gptconf)\n",
    "B,T=2,4\n",
    "x= torch.Tensor(\n",
    "    [\n",
    "        [12, 1, 43,44],\n",
    "        [gptconf.vocab_size-1,23,2,173]\n",
    "    ]\n",
    ").int()\n",
    "x = mock_emb(x)\n",
    "print(x.shape)\n",
    "# Attention mask (Prompt 1: all real, Prompt 2: 1 padding)\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1,1],  # Prompt 1: 4 real tokens\n",
    "    [0, 1, 1,1]   # Prompt 2: 1 padding, 3 real tokens\n",
    "], dtype=torch.float32)\n",
    "\n",
    "attention_model=CausalSelfAttention(gptconf)\n",
    "att,attention_mask,y = attention_model(x,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
